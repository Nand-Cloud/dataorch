{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604a3133-f50f-4bea-9c2b-7d7b0434ca91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1ï¸âƒ£ Start Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerDataCleaning\").getOrCreate()\n",
    "\n",
    "# 2ï¸âƒ£ Load your CSV file (update the actual file location if different)\n",
    "df = spark.read.csv(\"customer_data_partitioned.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"âœ… Raw data loaded\")\n",
    "df.show(5, truncate=False)\n",
    "print(f\"Total rows before cleaning: {df.count()}\")\n",
    "\n",
    "# 3ï¸âƒ£ Remove duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "print(f\"âœ… After dropping duplicates: {df.count()} rows\")\n",
    "\n",
    "# 4ï¸âƒ£ Standardize column names (lowercase, underscores)\n",
    "df = df.toDF(*[c.lower().strip().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "# 5ï¸âƒ£ Identify numeric & categorical columns\n",
    "numeric_cols = [f.name for f in df.schema.fields if \"StringType\" not in str(f.dataType)]\n",
    "categorical_cols = [f.name for f in df.schema.fields if \"StringType\" in str(f.dataType)]\n",
    "\n",
    "# 6ï¸âƒ£ Handle missing values\n",
    "# Fill numeric nulls with column mean\n",
    "for col in numeric_cols:\n",
    "    mean_val = df.select(F.mean(F.col(col))).collect()[0][0]\n",
    "    if mean_val is not None:\n",
    "        df = df.fillna({col: mean_val})\n",
    "\n",
    "# Fill categorical nulls with \"Unknown\"\n",
    "for col in categorical_cols:\n",
    "    df = df.fillna({col: \"Unknown\"})\n",
    "\n",
    "# 7ï¸âƒ£ Clean categorical columns (trim spaces, capitalize words)\n",
    "for col in categorical_cols:\n",
    "    df = df.withColumn(col, F.trim(F.col(col)))\n",
    "    df = df.withColumn(col, F.initcap(F.col(col)))  # Optional title case\n",
    "\n",
    "# 8ï¸âƒ£ Remove outliers (IQR method) for numeric columns\n",
    "for col in numeric_cols:\n",
    "    if df.filter(F.col(col).isNotNull()).count() > 0:\n",
    "        q1, q3 = df.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "        if q1 is not None and q3 is not None:\n",
    "            IQR = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * IQR\n",
    "            upper_bound = q3 + 1.5 * IQR\n",
    "            df = df.filter((F.col(col) >= lower_bound) & (F.col(col) <= upper_bound))\n",
    "\n",
    "# 9ï¸âƒ£ (Example) Remove invalid values for specific fields\n",
    "if \"age\" in numeric_cols:\n",
    "    df = df.filter((F.col(\"age\") >= 0) & (F.col(\"age\") <= 120))\n",
    "\n",
    "# ðŸ”Ÿ Save the cleaned data\n",
    "df.write.csv(\"cleaned_customer_data_partitioned.csv\", header=True, mode='overwrite')\n",
    "\n",
    "print(\"âœ… Data cleaning completed\")\n",
    "df.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-08-12 19:56:38",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
